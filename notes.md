 ### Tell me about yourself
 Thank you, my name is Remmy Bobga and I'm an experienced Cloud and DevOps Engineer with a passion for technology and a strong background in Cloud Computing, Cloud Security and DevOps.
 i
  - Throughout my career, I have developed my skills in utilizing various tools and processes to tackle organizational challenges. 

  - I have experience with cloud platforms such as AWS GCP and Azure, containerization through Docker, and container orchestration with Kubernetes.
  
  - Also, I am proficient in infrastructure provisioning using Terraform and other cloud-native tools, as well as configuration management with Ansible, Azure Automation and AWS System Manager.

  - I have experience in creating YAML, GROOVY, JSON, HCL, Bash, and Python scripts to automate deployment, monitoring, and configuration of applications, reducing manual efforts and ensuring consistent results.

  - I am an analytical problem-solver with a history of providing innovative and efficient solutions to enhance operational and system reliability. As a strong communicator and team player, I excel in collaborative settings where I can contribute towards achieving shared objectives.

  - As a team leader, I have mentored junior IT staff and ensured smooth onboarding processes as well as exposing them to my tech stack and experiences.

  - I am excited about the opportunity to bring my skills and expertise to your organization and help drive your success.

### About VanCity

  #### what VanCity does
  Vancity is a values-based financial co-operative that uses its assets to improve the financial well being of its members as well as their communities.

  #### My day to day tasks at VanCity
  At VanCity, I have worked on establishing and maintaining CI/CD pipelines using Jenkins for infrastructure deployment and Java projects. I have integrated various tools such as Terraform, Slack, Checkov scan, Git/GitHub, Maven, SonarQube, Nexus, Ansible, Docker, and Kubernetes for streamlined development, deployment, container orchestration, and security compliance. 

  I have migrated servers, databases, and workloads from on-premises to AWS using services like AWS Server Migration Service, AWS Application Migration Service, and AWS Data Migration Service (DMS). Additionally, I have created and maintained Dockerfiles for custom docker image building, and managed private image repositories in Docker Hub and Amazon ECR.

  Some other activities of mine at VanCity involved me setting up secure and scalable cloud-based Kubernetes clusters with Amazon EKS, Kops, and Kubeadm for automating deployment, scaling, and management of containerized microservices. I have utilized Kubernetes objects like Deployments, ReplicaSets, DaemonSets, ConfigMaps, Pods, Secrets, Volumes, and Namespaces.

  I have also created and managed Ansible Playbooks for configuring web application servers like Tomcat, Nginx, and Apache, handling user and package management, executing Cron jobs, and performing server upgrades and deployments. Additionally, I have utilized Helm for configuring monitoring, alerting, and logging tools like Prometheus, Grafana, Elasticsearch, Logstash, Filebeat, and Kibana, enabling application metrics tracking and logs management for data analytics.

  I also used Bash, Python and YAML scripting to automate tasks and processes for web applications, such as automating installation and configuration on Linux-based systems.

  That is a summary of my activities or tasks at Vancity

  #### Team size
  I am part of a dynamic core team of 6 DevOps professionals. We have a DevOps Lead and the other 5 of us are just sole contributors, we all report to our Team Lead or manager. Our team structure fostered collaboration, and we worked closely with other departments, such as development, QA, and product management, which made for a holistic approach to project execution.

  #### why you want to leave VanCity
  My contract at VanCity is coming to an end and I'm now seeking a more stable fulltime role with new challenges and opportunities to apply my skills in diverse environments and further broaden my horizons in the DevOps and cloud domains.

### About Intrado

  #### what Intrado does
  Intrado delivers safety-driven technology solutions backed by over 40 years of expertise to connect those in need with people who can best help them.

  #### Day to day task at Intrado.
  As a cloud engineer at Intrado, I had hands-on experience with Terraform and CloudFormation for Infrastructure as Code provisioning on AWS. I worked with various AWS services such as IAM, VPCs, EKS, EC2 instances, ELB, Auto-scaling Groups, Security Groups, Secret Manager, KMS, Certificate Manager, S3 buckets, EBS, RDS, and Route53 to provision and manage infrastructure in a scalable and efficient manner.

  I automated the provisioning of a three-tier monolithic application architecture using Terraform and Jenkins. This automation helped to eliminate manual configuration and deployment steps for a more efficient process. Additionally, I recommended and implemented cost-saving strategies for all AWS accounts. I achieved this by automating processes using AWS Instance Scheduler, CloudWatch events, DynamoDB tables, and Lambda functions, ensuring that EC2 and RDS instances in development and staging environments are automatically stopped during scheduled hours. This approach helped to reduce unnecessary costs associated with running instances outside of business hours.

  I monitored and analyzed the performance of cloud-based applications across various regions using application performance monitoring tools like Splunk. This proactive approach ensured timely visualization, detection, and resolution of issues as they emerged.

  I implemented security best practices by utilizing security groups, network ACLs, encryption, and identity and access management (IAM). This helped to ensure data protection and adherence to compliance requirements. By incorporating security best practices into my work, I helped to minimize the risk of security breaches and data loss.

  Overall, my experience as a cloud engineer at Intrado involved utilizing various AWS services and tools to provision, manage, monitor, and secure cloud infrastructure. I demonstrated a strong understanding of infrastructure as code, automation, cost optimization, performance monitoring, and security best practices, all of which are essential skills for a cloud engineer.

  #### Team size  
  I was a member of a close-knit DevOps team consisting of 8 professionals. Our size enabled us to be agile, rapidly adapting to changes and implementing solutions. While our core team was relatively small, we collaborated extensively with other departments like software development, customer support, and sales to ensure seamless integration of our solutions across the company.

  #### why did you leave Intrado
  My time at Intrado was truly rewarding, both in terms of skill development and the projects I got to work on. 
    
  However, after two years, My contract came to an end so i had to find some other opportunity.


### what are you looking for in your next role/organisation
1. **Innovative Cloud Environment:**
   I'm seeking a company that fosters innovation in cloud technologies. An environment that encourages the exploration and implementation of cutting-edge solutions in cloud computing, ensuring that I can contribute to and stay abreast of the latest industry advancements.

2. **Collaborative DevOps Culture:**
   I value a collaborative DevOps culture where cross-functional teams work seamlessly to achieve common goals. I'm looking for a company that appreciates the synergy between development and operations, emphasizing collaboration, communication, and the implementation of DevOps best practices.

3. **Continuous Learning Opportunities:**
   I'm eager to work for a company that invests in the professional development of its employees. Whether through training programs, certifications, or exposure to diverse projects, I am looking for an environment that supports continuous learning and growth in the ever-evolving landscape of DevOps and cloud technologies.


### A project where you faced some difficulties and how you solved the issue 
In a recent project at Vancity, we encountered challenges during the migration of a critical three-tier monolithic application to Azure. One of the major hurdles was ensuring a smooth transition without disrupting the existing services.

The primary difficulty arose from the complexity of the application architecture, involving multiple interconnected components. This complexity increased the risk of data inconsistencies and potential downtime during the migration process.

To address this, I implemented a phased migration approach tailored to the Azure environment. We conducted a thorough analysis of the application's dependencies and data flow within Azure services. We prioritized and migrated individual components in a carefully planned sequence, ensuring that each phase was thoroughly tested before moving on to the next.

Leveraging Azure DevOps for CI/CD, we automated the provisioning and deployment processes using Azure Resource Manager (ARM) templates. This not only reduced the likelihood of manual errors but also ensured consistency across the Azure environment.

Continuous monitoring and collaboration with the development and operations teams were crucial during this process. We used Azure Monitor for real-time monitoring and established clear communication channels to promptly address any issues that arose.

This approach not only mitigated the risks associated with the Azure migration but also allowed for quick identification and resolution of any unforeseen challenges. The phased migration strategy, coupled with automation and constant communication, ensured a successful transition to Azure while minimizing disruption to the business-critical application.

### talk about a time when an application went down such that it wasn't accessible to users and how did you solve the issue
At Vancity, I encountered a critical situation when an application became inaccessible to users due to unexpected issues. The incident demanded swift action to minimize downtime and restore normal operations.

Upon immediate investigation, we identified that the root cause was a combination of increased user load and an unforeseen spike in database transactions, causing a performance bottleneck. The initial challenge was to diagnose the issue promptly and implement a solution to restore service availability.

I addressed the situation by:

1. **Immediate Triage:**
   I collaborated with the operations and development teams to conduct a thorough analysis of the incident. We used monitoring tools, including Azure Application Insights and Azure Monitor, to identify the specific areas of the application affected and the performance metrics that deviated from the norm.

2. **Scaling Resources:**
   Recognizing the increased demand, I initiated the immediate scaling of resources on Azure, both horizontally and vertically. This involved dynamically adjusting the number of application instances and enhancing the capacity of the underlying infrastructure. Azure's auto-scaling features and Virtual Machine Scale Sets were instrumental in achieving this swiftly.

3. **Database Optimization:**
   Simultaneously, I worked closely with the database team to optimize database queries and indexes. We utilized Azure SQL Database performance tuning features and executed query optimization strategies to alleviate the database load. This proactive step helped in addressing the root cause of the performance bottleneck.

4. **Communication and Transparency:**
   Throughout the resolution process, I maintained transparent communication with stakeholders, providing real-time updates on the progress and expected resolution time. Clear and concise messages were shared through channels like Azure Status Page, emails, and internal communication platforms.

By taking these swift and targeted actions, we were able to restore the application's accessibility to users in a short timeframe. The incident not only highlighted the importance of proactive monitoring but also underscored the significance of a well-defined incident response plan and collaboration across teams to ensure minimal impact on user experience.

### talk about a time you have to explain a technical situation to a non technical person
At Vancity, I had an experience where I needed to explain a complex technical situation to a non-technical stakeholder, specifically regarding the implementation of a new CI/CD pipeline for an application.

I approached it by:

1. **Understanding the Audience:**
   Before diving into the technical details, I took a moment to understand the stakeholder's background and familiarity with technical concepts. This helped me tailor my explanation to their level of understanding and avoid unnecessary jargon.

2. **Use of Analogies:**
   To simplify the concept of a CI/CD pipeline, I drew parallels with everyday scenarios. I compared the pipeline to a well-organized assembly line in a factory, where each stage represents a specific task in the software development and deployment process. This analogy made it easier for the non-technical person to visualize the flow and purpose of the CI/CD pipeline.

3. **Visual Aids:**
   I created simple diagrams and flowcharts to illustrate the CI/CD process visually. Visual aids are powerful tools for conveying complex technical concepts in a more digestible format. The diagrams helped in breaking down the pipeline stages and showcasing how code moves from development to deployment.

4. **Focus on Business Impact:**
   Instead of delving into intricate technical details, I emphasized the business benefits of implementing a CI/CD pipeline. I highlighted how it accelerates software delivery, reduces manual errors, and ultimately contributes to faster and more reliable application releases. Connecting technical details to tangible business outcomes helped the stakeholder see the value in the implementation.

5. **Encouraging Questions:**
   I encouraged the stakeholder to ask questions throughout the explanation. This not only ensured their engagement but also allowed me to address any specific concerns or uncertainties they might have had. It fostered a more interactive and collaborative discussion.

By combining these strategies, I was able to successfully convey the essence of the CI/CD pipeline to the non-technical stakeholder. The key was to bridge the gap between technical intricacies and their real-world impact, making the information more accessible and relevant to the audience.

AWS SERCURITY
